\section{Introduction}\label{sec:intro}
%\begin{itemize}
%	\item Generally introduce dialog acts
%	\item Need for automated tagging
%	\item Current SotA
%	\item Introduce distributional representations
%	\item Present hypotheses
%	\item ...
%	\item Set up rest of report
%\end{itemize}
Discourse structure analysis is essential for understanding spontaneous dialogs and developing human-computer dialog systems.
An essential part of discourse structure analysis is the identification of dialog act classes (e.\ g.\ \emph{questions}, \emph{self-talks}, \emph{statements}, \emph{backchannels}).
As defined by Austin \shortcite{john1962austin}, dialog acts present linguistic abstractions of the ilocutionary force of speech acts and model the communicative intention of an utterance in a conversation.
There are several tasks that require utterances to be tagged with dialog acts.
Examples of these include speech recognition, speech synthesis, summarization, and of course, human-machine dialog systems.
The correct identification of dialog act tags for utterances is thus an important research topic.


Table~\ref{tab:swda_example} shows an example of dialog acts from the Switchboard corpus we are trying to classify.
The table already gives an idea that some of the 42 dialog acts might have a rather closed set of possible realisations (e.g. the classes \emph{Agree} or \emph{Acknowledge}) whereas classes like \emph{Statement} can contain utterances of almost any content.
Although the individual words in an utterance are important cues, we argue that the ilocutionary force, and thus the dialog act tag, of an utterance is derived from: 1) the \emph{words} in that utterance, 2) their \emph{composition} and 3) the \emph{context} of the dialog as a whole. 



\begin{table}[h]
\centering
\small
\begin{tabular}{ll}
\hline
\textbf{Tag} & \textbf{Speaker / Utterance}
\\
\hline
Wh-Question & A: how old is your daughter?\\
Statement-non-opinion & B: she's three.\\
Summarize & A: oh , so she's a little one.\\
Agree & B: yes.\\
Acknowledge & A: yeah.\\
Statement-non-opinion & B: she's, she's little.\\
\hline
\end{tabular}
\caption{SWDA dialog excerpt.}
\label{tab:swda_example}
\end{table}

For the purpose of this work, we will focus on capturing the first two of these three aspects. We extract feature representations for complete, but isolated, utterances in an unsupervised fashion.
We therefore build a distributional semantic model that learns vector representations for entire utterances and then use these vector features as inputs for different machine learning classifiers, expecting that the embeddings can model the meaning of an entire utterance.
Several techniques can be used for mapping text units to a high dimensional real value space.
Utterance embeddings have the attractive property of representing an entire textual sequence as a vector while taking word order into account, as opposed to the classical \emph{Bag-of-words} approach in which word order is not preserved and in which resulting vectors show only little semantic relations.
We expect this additional information to play an important role in the classification task. In order to infer those embeddings we use the \emph{paragraph2vec} framework recently introduced by \newcite{le2014distributed}, which is based on the earlier word embedding models by \newcite{mikolov2013efficient}.

For the actual dialog act tagging we treat the problem as a multi-class classification task and classify utterances both in isolation as well as in the context of the previous utterances.
We evaluate the tagging accuracy and compare different models.
Besides the baselines provided by research from previous work, we compare the results against a simple baseline that uses a bag-of-words (BOW) representation for each utterance. 

The outline of this report is set as follows: In Section 2, we present relevant approaches that aim at classifying dialog acts and briefly describe their main characteristics and results. In Section 3 we describe the concept of utterance embeddings and their training in more detail. Section 4 specifies the datasets and the details of our classification pipeline. The results of the experiments are presented in Section 5. Finally, Section 6 includes conclusions drawn from this work as well as issues and future work. \lau{check}
