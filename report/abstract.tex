\begin{abstract}
%	\begin{itemize}
%		\item Dialog act (tagging)
%		\item Word/sentence embedding
%		\item Our research in one sentence
%		\item Results and compare to others
%		\item ...
%	\end{itemize}
In this work, we experiment with a new feature representation for dialog act tagging by learning distributional embeddings for utterances.
We train a distributional semantic model that allows us to infer vector representations for entire utterances in an unsupervised fashion.
These representations are used to train several classifiers in order to tag utterances in the Switchboard corpus.
We show that these utterance embeddings can be used for dialog act tagging. Our method, however, is outperformed by a simple bag-of-words baseline.
%\lau{add something about results}
\end{abstract}
