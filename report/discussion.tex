\section{Conclusion}\label{sec:conclusion}
We see that all models trained on utterance embeddings found with the techniques from \newcite{le2014distributed} outperform the trivial baseline of predicting the most frequent tag. This supports the hypothesis that such distributional representations can be used successfully for dialog act tagging.

However a simple model using a bag of words representation and a Naive Bayes classifier outperforms these more intricate representations and models. \david{why is this? because of the short utterances? seems strange...}

Moreover we see that training the utterance embeddings with extra samples from the BNC does not have a big effect on the accuracy of the classifiers. This could be explained by the fact that the utterances in the BNC have been cleaned to be more like grammatically correct sentences, where the SwDA is in comprised of more accurately transcribed utterances. Therefore we recommend future work to use additional data that is more similar to the data used in training and evaluating the classifier. In order to investigate whether this increases the accuracy of the tagging.

When comparing the results of the classifiers on a coarser tag set we see that the accuracy dramatically increases for all classifiers. This shows that the classifiers make most mistakes confusing dialog acts that are closely related. However the same goes for our baseline which has a similar increase in accuracy when predicting on a coarser tag set. Therefore we are not able to beat the baseline on a coarser tag set either.

Perhaps the most prominent deficiency of the approach presented in this paper is the lack of naturally incorporating context into the utterance representation. Using only the previous utterance is not enough to model the context of the current utterance. Even though this behaviour was to be expected, it is surprising that aggregating the previous embedding (either by concatenation or addition) did not improve accuracy.\david{Can somebody check this?} Future work should therefore mainly concentrate on mending this weakness. One possible approach to this problem would be to use yet another deep learning neural network technique called Long Short Term Memory (LSTM) as proposed by \newcite{lstm-original}. This recurrent neural network uses a so called memory cell with four connections, an input gate, an output gate, a self recurrent gate and a forget gate. The input gate can be used to alter the state of the memory cell, while the output gate can be used to affect the state of other layers. This technique can be applied to dialog act tagging by using the utterance embeddings presented earlier as inputs for the LSTM. The output of the LSTM cell can then be used to predict a tag for that utterance. Since the utterances are fed to the LSTM in sequence, the memory cell can be thought to represent the contextual knowledge of the conversation up to that point.

From our initial hypothesis, we were able to show that: (a) Utterance embeddings capture relevant features given the words that compose them and can be used in dialog act classification; (b) The multiple experiments we run, varying training data, classifiers, and hyperparameters for all components, demonstrate that our approach is not sufficient and a Bag-of-Words based method can outperform it; (c) Using additional data to get the utterance embeddings does not consistently yield better results, the final performance varies per classification technique, and improvements are not of a notable magnitude.

\david{are we missing anything?}
\david{some general conclusion to wrap it all up here.}