\section{Methodology}\label{sec:method}
%\david{Figure out how we want to present this clearly as we use a lot of `black boxes'}
%\david{We might want to rename this to utterance embeddings? And then hace a seperate section on the actual methodology?}
%
%\begin{itemize}
%	\item How do we find utterance embeddings?
%	\item Use these as features
%	\item For ML algorithms (NN, SVM, NB?, Nearest Neighbor)
%	\item maybe here something on datasets?
%	\item ...
%\end{itemize}
%\david{This needs a brief introduction with set up of this section}

We evaluate our utterance embedding features for dialog act taggin in two different settings:
1) We tag each utterance independent of the context it appears in.
2) We include the context of the previous utterance by concatenating the utterance embeddings of the previous utterance and the current utterance to obtain a very simplified discourse model as discussed in Section 2.
In addition, we experiment with two different tagset granulairities.
The dataset, the actual classifiers and the tagsets are described in the following two subsections.
\roger{Baselines}

\subsection{Dialog datasets}
The dialog act classifiers are trained in a supervised fashion, which required labelled data.
For that purpose, we evaluate the actual dialog act tagging on the Switchboard Dialog Act corpus (SwDA) \roger{cite}.
The corpus is a compilation of telephone transcripts between two interlocutors.
It contains a total of 205,000 utterances and 1.4 million words.
Each utterance is assigned one of 42 possible dialog act tags (according to teh Discourse Annotation and Markup System of Labelling - DAMSL).
In our experiments, we also use a coarser tagset where we manually map the original tags into \roger{how many?} classes. The mapping is described in \roger{appendix or where?}.
Table \ref{tab:swda_example} shows an example of a small excerpt from the corpus with the respective dialog act labels. 

We use the same training and test split as described in Stolcke \shortcite{stolcke2000}, which uses 1,115 dialogs for training and 19 dialogs for testing and which has been widely used in previous work.
The utterances are preprocessed by removing linguistic annotations and any interpunctuation except for periods, question marks and exclamation marks.
Additionally, utterances that complete previously interrupted utterences, which are marked with a continuation tag \texttt{+}, were concatenated with their initial segment, which also contains the correct dialog act label for the complete utterance.
That preprocessing step was also performed by \newcite{milajevs}.

As mentioned before, the training of the utterance embeddings is completely unsupervised and does not require any labelled data.
For that reason, we experiment with expanding our data for the training of the utterance representations.
In addition to the Switchboard corpus, we use the spoken dialog data from the British National Corpus (BNC) \roger{cite}.
The resulting dataset from the BNC contains approximately 1 million utterances. 
The BNC utterances, however, different inherently from the Switchboard utterances as they are usually longer and normalized to full grammatical sentences.
The utterances in the SWDA dataset are more fragmentary and thus usually shorter.


%The Switchboard Dialog Act corpus (SwDA) consists of a compilation of telephone transcripts between two interlocutors.
%It contains a total 205.000 utterances and 1.4 million words
%Each utterance is associated to a label, which summarizes syntactic, semantic, and pragmatic information.
%The corpus contains a total of 200 tags, which can be further aggregated into 44 main classes. Table~\ref{tab:swda_tag_example} shows examples for the five most common tags. Table~\ref{tab:swda_sent_example} present examples of utterances contained in the SwDA corpus.

%\begin{table}[h]
%\centering
%\begin{tabular}{l l l c}
%\hline
%\textbf{Tag} & \textbf{Description} & \textbf{Example} & \textbf{\%}\\
%\hline
%st & Statement-non-opinion & Me, I'm in the & 36 \\
% & & legal department. & \\
%b & Acknowledge & Acknowledge Uh-huh. & 19 \\
%sv & Statement-opinion & I think it's great. & 13 \\
%aa & Accept & That's exactly it. & 5 \\
%\% & Turn exit & So,- & 5 \\
% \hline
%\end{tabular}
%\caption{SWDA's 5 most frequent tags.}
%\label{tab:swda_tag_example}
%\end{table}

%\begin{table}[h]
%\centering
%\begin{tabular}{p{0.45\textwidth}}
%\hline
%qrr B.34.utt2: {C or } do you think that [ we're, + we're, ] {F uh, } all trying to keep up with a certain standard of living? \\
%sv A.35.utt1: I think that's part of it too. \\
%sv A.35.utt2: {C But } I do think, - \\
%qy B.36.utt1: {E I mean } do you think, \\
%\hline
%\end{tabular}
%\caption{SwDA utterance examples.}
%\label{tab:swda_sent_example}
%\end{table}

% \subsubsection*{The British National corpus}
% The British National corpus (BNC) is a collection of 100 million words sampled from different written and spoken sources, with the intention of representing the British English language. Some of the sources of these data include newspapers, articles, journals, books, letter, transcription of informal conversations, among others. This dataset contains a huge amount of unlabelled sentences.
% Table~\ref{tab:bnc_sent_example} presents sentence examples extracted from the BNC.

% \begin{table}[h]
% \centering
% \begin{tabular}{p{0.45\textwidth}}
% \hline
% ADR 172 The kind of girl that even if you didn't know well you always said "hello" to and got a cheery wave and a smile back. \\
% B72 966 For example, the sedimentary rocks that form the top geological layer in much of southern Britain may be only a few hundred metres thick in a few isolated sites. \\
% B2E 714 Then Fulham got one of her worst raids of the war. \\
%  \hline
% \end{tabular}
% \caption{BNC sentence examples.}
% \label{tab:bnc_sent_example}
% \end{table}

% \subsection{Discourse Model}
% \david{Please check the following on readability}

% Just like \newcite{stolcke2000} we will model a dialog as an HMM. The hidden states will be the dialog acts and the observed quantities the uttered words and their speaker. To find the most likely tag for a single utterance we will find the best dialog act tag $t$ for an utterance using equation \ref{eq:maxtag}.

% \begin{equation}\label{eq:maxtag}
% 	\begin{align*}
% 	t^* &= \underset{t}{argmax} P(t|u) \\
% 	&=\underset{t}{argmax} \frac{P(u|t)P(t)}{P(u)}\\
% 	&=\underset{t}{argmax} \underbrace{P(u|t)}_{\text{sentence model}}\underbrace{P(t)}_{\text{discourse model}}\\
% 	\end{align*}
% \end{equation}

% We discuss the sentence model in detail in section \ref{sec:sentencemodel}. As a discourse model we will assume that the \emph{prior probability} of a tag depends on the preceding tags and their speakers. $P(t_i) = P(t_i|t_{i-1}...t_0, s_{i-1}...s_0)$. Where we assume that the discourse is a Markov Model of order $k$: $P(t_i) = P(t_i|t_{i-1}...t_{i-k}, s_{i-1}...s_{i-k})$.

% Different dynamic programming techniques can be used to find the most probable sequence of tags $\mathbf{t}$ given a sequence of utterances $\mathbf{u}$ and their speaker $\mathbf{s}$, we will use the Viterbi decoding algorithm \newcite{viterbi}.

% \david{the following could go to the discussion, but i think it is good to note these assumptions straight away}
% An assumption made in this model is that two sequential utterances $u_i$ and $u_{i+1}$ are independent of each other. We know that in conversation interlocutors tend to align on different levels including lexical choices \newcite{echoes}. Also it is very likely that the same words will be used in the answer to a question as in the question. However we count on the fact that the dependence of $u_i$ and $t_i$ will be stronger than the is stronger than the independence violation of $u_i$ and $u_{i+1}$ \newcite{stolcke2000}.


% \subsection{Sentence Models}\label{sec:sentencemodel}
% \david{Do we want to do this before or after the discourse model?}


\subsection{Classifiers}
\lau{Mention which classifiers we use, and with which params}
For the actual classification task we use three different classifiers which we treat as ``black-boxes,'' i.\ e.\ we do not attempt to evaluate their hyperparameters.
